{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd886a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2025 Raphael Yana\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0df47db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ENV_VARS = {\n",
    "    \"GOOGLE_KEY\": \"Google Gemini\",\n",
    "}\n",
    "\n",
    "for var, name in ENV_VARS.items():\n",
    "    if not os.getenv(var):\n",
    "        raise ValueError(f\"Missing {name} API key: `{var}` must be set in the environment.\")\n",
    "    \n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b67fac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === Processing Republic...\n",
      "→ Filtering boundaries for Republic …\n",
      "\n",
      "–– DETECTED TABLE OF CONTENTS ––\n",
      "\n",
      "INTRODUCTION AND ANALYSIS.\n",
      " THE REPUBLIC.\n",
      " PERSONS OF THE DIALOGUE.\n",
      " BOOK I.\n",
      " BOOK II.\n",
      " BOOK III.\n",
      " BOOK IV.\n",
      " BOOK V.\n",
      " BOOK VI.\n",
      " BOOK VII.\n",
      " BOOK VIII.\n",
      " BOOK IX.\n",
      " BOOK X.\n",
      "\n",
      "✓ LLM picked narrative start at byte 553683\n",
      "Structure profile written → utils/struct_profile_republic.json\n",
      "  level_0: 1 headings (showing 3) → ['INTRODUCTION AND ANALYSIS.']\n",
      "  level_1: 12 headings (showing 3) → ['THE REPUBLIC.', 'PERSONS OF THE DIALOGUE.', 'BOOK I.']\n",
      "Cleaned text saved to data/republic/cleaned.txt\n",
      "→ Splitting into structured paragraphs …\n",
      "Detected TOC file found, using it to build structure profile.\n",
      "Saved 4359 structured paragraphs → data/republic/paragraphs.jsonl\n",
      "→ Classifying paragraphs …\n",
      "=== Avg words per paragraph: 27.0, raw chunk size: 74, bounded to: 50\n",
      "→ Classifying chunk 1/88: paragraphs 0–49\n",
      "→ Classifying chunk 2/88: paragraphs 50–99\n",
      "→ Classifying chunk 3/88: paragraphs 100–149\n",
      "→ Classifying chunk 4/88: paragraphs 150–199\n",
      "→ Classifying chunk 5/88: paragraphs 200–249\n",
      "→ Classifying chunk 6/88: paragraphs 250–299\n",
      "→ Classifying chunk 7/88: paragraphs 300–349\n",
      "→ Classifying chunk 8/88: paragraphs 350–399\n",
      "→ Classifying chunk 9/88: paragraphs 400–449\n",
      "→ Classifying chunk 10/88: paragraphs 450–499\n",
      "→ Classifying chunk 11/88: paragraphs 500–549\n",
      "→ Classifying chunk 12/88: paragraphs 550–599\n",
      "→ Classifying chunk 13/88: paragraphs 600–649\n",
      "→ Classifying chunk 14/88: paragraphs 650–699\n",
      "→ Classifying chunk 15/88: paragraphs 700–749\n",
      "→ Classifying chunk 16/88: paragraphs 750–799\n",
      "→ Classifying chunk 17/88: paragraphs 800–849\n",
      "→ Classifying chunk 18/88: paragraphs 850–899\n",
      "→ Classifying chunk 19/88: paragraphs 900–949\n",
      "→ Classifying chunk 20/88: paragraphs 950–999\n",
      "→ Classifying chunk 21/88: paragraphs 1000–1049\n",
      "→ Classifying chunk 22/88: paragraphs 1050–1099\n",
      "→ Classifying chunk 23/88: paragraphs 1100–1149\n",
      "→ Classifying chunk 24/88: paragraphs 1150–1199\n",
      "→ Classifying chunk 25/88: paragraphs 1200–1249\n",
      "→ Classifying chunk 26/88: paragraphs 1250–1299\n",
      "→ Classifying chunk 27/88: paragraphs 1300–1349\n",
      "→ Classifying chunk 28/88: paragraphs 1350–1399\n",
      "→ Classifying chunk 29/88: paragraphs 1400–1449\n",
      "→ Classifying chunk 30/88: paragraphs 1450–1499\n",
      "→ Classifying chunk 31/88: paragraphs 1500–1549\n",
      "→ Classifying chunk 32/88: paragraphs 1550–1599\n",
      "→ Classifying chunk 33/88: paragraphs 1600–1649\n",
      "→ Classifying chunk 34/88: paragraphs 1650–1699\n",
      "→ Classifying chunk 35/88: paragraphs 1700–1749\n",
      "→ Classifying chunk 36/88: paragraphs 1750–1799\n",
      "→ Classifying chunk 37/88: paragraphs 1800–1849\n",
      "→ Classifying chunk 38/88: paragraphs 1850–1899\n",
      "→ Classifying chunk 39/88: paragraphs 1900–1949\n",
      "→ Classifying chunk 40/88: paragraphs 1950–1999\n",
      "→ Classifying chunk 41/88: paragraphs 2000–2049\n",
      "→ Classifying chunk 42/88: paragraphs 2050–2099\n",
      "→ Classifying chunk 43/88: paragraphs 2100–2149\n",
      "→ Classifying chunk 44/88: paragraphs 2150–2199\n",
      "→ Classifying chunk 45/88: paragraphs 2200–2249\n",
      "→ Classifying chunk 46/88: paragraphs 2250–2299\n",
      "→ Classifying chunk 47/88: paragraphs 2300–2349\n",
      "→ Classifying chunk 48/88: paragraphs 2350–2399\n",
      "→ Classifying chunk 49/88: paragraphs 2400–2449\n",
      "→ Classifying chunk 50/88: paragraphs 2450–2499\n",
      "→ Classifying chunk 51/88: paragraphs 2500–2549\n",
      "→ Classifying chunk 52/88: paragraphs 2550–2599\n",
      "→ Classifying chunk 53/88: paragraphs 2600–2649\n",
      "→ Classifying chunk 54/88: paragraphs 2650–2699\n",
      "→ Classifying chunk 55/88: paragraphs 2700–2749\n",
      "→ Classifying chunk 56/88: paragraphs 2750–2799\n",
      "→ Classifying chunk 57/88: paragraphs 2800–2849\n",
      "→ Classifying chunk 58/88: paragraphs 2850–2899\n",
      "→ Classifying chunk 59/88: paragraphs 2900–2949\n",
      "→ Classifying chunk 60/88: paragraphs 2950–2999\n",
      "→ Classifying chunk 61/88: paragraphs 3000–3049\n",
      "→ Classifying chunk 62/88: paragraphs 3050–3099\n",
      "→ Classifying chunk 63/88: paragraphs 3100–3149\n",
      "→ Classifying chunk 64/88: paragraphs 3150–3199\n",
      "→ Classifying chunk 65/88: paragraphs 3200–3249\n",
      "→ Classifying chunk 66/88: paragraphs 3250–3299\n",
      "→ Classifying chunk 67/88: paragraphs 3300–3349\n",
      "→ Classifying chunk 68/88: paragraphs 3350–3399\n",
      "→ Classifying chunk 69/88: paragraphs 3400–3449\n",
      "→ Classifying chunk 70/88: paragraphs 3450–3499\n",
      "→ Classifying chunk 71/88: paragraphs 3500–3549\n",
      "→ Classifying chunk 72/88: paragraphs 3550–3599\n",
      "→ Classifying chunk 73/88: paragraphs 3600–3649\n",
      "→ Classifying chunk 74/88: paragraphs 3650–3699\n",
      "→ Classifying chunk 75/88: paragraphs 3700–3749\n",
      "→ Classifying chunk 76/88: paragraphs 3750–3799\n",
      "→ Classifying chunk 77/88: paragraphs 3800–3849\n",
      "→ Classifying chunk 78/88: paragraphs 3850–3899\n",
      "→ Classifying chunk 79/88: paragraphs 3900–3949\n",
      "→ Classifying chunk 80/88: paragraphs 3950–3999\n",
      "→ Classifying chunk 81/88: paragraphs 4000–4049\n",
      "→ Classifying chunk 82/88: paragraphs 4050–4099\n",
      "→ Classifying chunk 83/88: paragraphs 4100–4149\n",
      "→ Classifying chunk 84/88: paragraphs 4150–4199\n",
      "→ Classifying chunk 85/88: paragraphs 4200–4249\n",
      "→ Classifying chunk 86/88: paragraphs 4250–4299\n",
      "→ Classifying chunk 87/88: paragraphs 4300–4349\n",
      "→ Classifying chunk 88/88: paragraphs 4350–4358\n",
      "=== Saved 2132 paragraphs to runs/republic/socrates_segments.jsonl\n",
      "✓ Done.\n",
      "✓ Usage report saved to runs/republic/usage_report.json\n",
      "\n",
      " === Processing Symposium...\n",
      "→ Filtering boundaries for Symposium …\n",
      " === No TOC file found — skipping TOC profile generation.\n",
      "Cleaned text saved to data/symposium/cleaned.txt\n",
      "→ Splitting into structured paragraphs …\n",
      "No TOC found – falling back to default structure profile.\n",
      "Saved 286 structured paragraphs → data/symposium/paragraphs.jsonl\n",
      "→ Classifying paragraphs …\n",
      "=== Avg words per paragraph: 150.9, raw chunk size: 13, bounded to: 13\n",
      "→ Classifying chunk 1/22: paragraphs 0–12\n",
      "→ Classifying chunk 2/22: paragraphs 13–25\n",
      "→ Classifying chunk 3/22: paragraphs 26–38\n",
      "→ Classifying chunk 4/22: paragraphs 39–51\n",
      "→ Classifying chunk 5/22: paragraphs 52–64\n",
      "→ Classifying chunk 6/22: paragraphs 65–77\n",
      "→ Classifying chunk 7/22: paragraphs 78–90\n",
      "→ Classifying chunk 8/22: paragraphs 91–103\n",
      "→ Classifying chunk 9/22: paragraphs 104–116\n",
      "→ Classifying chunk 10/22: paragraphs 117–129\n",
      "→ Classifying chunk 11/22: paragraphs 130–142\n",
      "→ Classifying chunk 12/22: paragraphs 143–155\n",
      "→ Classifying chunk 13/22: paragraphs 156–168\n",
      "→ Classifying chunk 14/22: paragraphs 169–181\n",
      "→ Classifying chunk 15/22: paragraphs 182–194\n",
      "→ Classifying chunk 16/22: paragraphs 195–207\n",
      "→ Classifying chunk 17/22: paragraphs 208–220\n",
      "→ Classifying chunk 18/22: paragraphs 221–233\n",
      "→ Classifying chunk 19/22: paragraphs 234–246\n",
      "→ Classifying chunk 20/22: paragraphs 247–259\n",
      "→ Classifying chunk 21/22: paragraphs 260–272\n",
      "→ Classifying chunk 22/22: paragraphs 273–285\n",
      "=== Saved 96 paragraphs to runs/symposium/socrates_segments.jsonl\n",
      "✓ Done.\n",
      "✓ Usage report saved to runs/symposium/usage_report.json\n",
      "\n",
      " === Processing Hamlet...\n",
      "→ Filtering boundaries for Hamlet …\n",
      "\n",
      "–– DETECTED TABLE OF CONTENTS ––\n",
      "\n",
      "ACT I\n",
      " Scene I. Elsinore. A platform before the Castle\n",
      " Scene II. Elsinore. A room of state in the Castle\n",
      " Scene III. A room in Polonius’s house\n",
      " Scene IV. The platform\n",
      " Scene V. A more remote part of the Castle\n",
      "\n",
      " ACT II\n",
      " Scene I. A room in Polonius’s house\n",
      " Scene II. A room in the Castle\n",
      "\n",
      " ACT III\n",
      " Scene I. A room in the Castle\n",
      " Scene II. A hall in the Castle\n",
      " Scene III. A room in the Castle\n",
      " Scene IV. Another room in the Castle\n",
      "\n",
      " ACT IV\n",
      " Scene I. A room in the Castle\n",
      " Scene II. Another room in the Castle\n",
      " Scene III. Another room in the Castle\n",
      " Scene IV. A plain in Denmark\n",
      " Scene V. Elsinore. A room in the Castle\n",
      " Scene VI. Another room in the Castle\n",
      " Scene VII. Another room in the Castle\n",
      "\n",
      " ACT V\n",
      " Scene I. A churchyard\n",
      " Scene II. A hall in the Castle\n",
      "\n",
      "✓ LLM picked narrative start at byte 2350\n",
      "Structure profile written → utils/struct_profile_hamlet.json\n",
      "  level_0: 1 headings (showing 3) → ['ACT I']\n",
      "  level_1: 24 headings (showing 3) → ['Scene I. Elsinore. A platform before the Castle', 'Scene II. Elsinore. A room of state in the Castle', 'Scene III. A room in Polonius’s house']\n",
      "Cleaned text saved to data/hamlet/cleaned.txt\n",
      "→ Splitting into structured paragraphs …\n",
      "Detected TOC file found, using it to build structure profile.\n",
      "Saved 1447 structured paragraphs → data/hamlet/paragraphs.jsonl\n",
      "→ Classifying paragraphs …\n",
      "=== Avg words per paragraph: 6.7, raw chunk size: 298, bounded to: 50\n",
      "→ Classifying chunk 1/29: paragraphs 0–49\n",
      "→ Classifying chunk 2/29: paragraphs 50–99\n",
      "→ Classifying chunk 3/29: paragraphs 100–149\n",
      "→ Classifying chunk 4/29: paragraphs 150–199\n",
      "→ Classifying chunk 5/29: paragraphs 200–249\n",
      "→ Classifying chunk 6/29: paragraphs 250–299\n",
      "→ Classifying chunk 7/29: paragraphs 300–349\n",
      "→ Classifying chunk 8/29: paragraphs 350–399\n",
      "→ Classifying chunk 9/29: paragraphs 400–449\n",
      "→ Classifying chunk 10/29: paragraphs 450–499\n",
      "→ Classifying chunk 11/29: paragraphs 500–549\n",
      "→ Classifying chunk 12/29: paragraphs 550–599\n",
      "→ Classifying chunk 13/29: paragraphs 600–649\n",
      "→ Classifying chunk 14/29: paragraphs 650–699\n",
      "→ Classifying chunk 15/29: paragraphs 700–749\n",
      "→ Classifying chunk 16/29: paragraphs 750–799\n",
      "→ Classifying chunk 17/29: paragraphs 800–849\n",
      "→ Classifying chunk 18/29: paragraphs 850–899\n",
      "→ Classifying chunk 19/29: paragraphs 900–949\n",
      "→ Classifying chunk 20/29: paragraphs 950–999\n",
      "→ Classifying chunk 21/29: paragraphs 1000–1049\n",
      "→ Classifying chunk 22/29: paragraphs 1050–1099\n",
      "→ Classifying chunk 23/29: paragraphs 1100–1149\n",
      "→ Classifying chunk 24/29: paragraphs 1150–1199\n",
      "→ Classifying chunk 25/29: paragraphs 1200–1249\n",
      "→ Classifying chunk 26/29: paragraphs 1250–1299\n",
      "→ Classifying chunk 27/29: paragraphs 1300–1349\n",
      "→ Classifying chunk 28/29: paragraphs 1350–1399\n",
      "→ Classifying chunk 29/29: paragraphs 1400–1446\n",
      "=== Saved 368 paragraphs to runs/hamlet/hamlet_segments.jsonl\n",
      "✓ Done.\n",
      "✓ Usage report saved to runs/hamlet/usage_report.json\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from pipeline import BookProcessingPipeline\n",
    "\n",
    "\n",
    "books = [\n",
    "    {\n",
    "        \"raw_txt\": \"data/republic.txt\",\n",
    "        \"cleaned_txt\": \"data/cleaned_republic.txt\",\n",
    "        \"book_name\": \"Republic\",\n",
    "        \"target_speaker\": \"Socrates\",\n",
    "        \"is_narrator\": True\n",
    "    },\n",
    "    {\n",
    "        \"raw_txt\": \"data/symposium.txt\",\n",
    "        \"cleaned_txt\": \"data/cleaned_symposium.txt\",\n",
    "        \"book_name\": \"Symposium\",\n",
    "        \"target_speaker\": \"Socrates\",\n",
    "        \"is_narrator\": False\n",
    "    },\n",
    "    {\n",
    "        \"raw_txt\": \"data/hamlet.txt\",\n",
    "        \"cleaned_txt\": \"data/cleaned_hamlet.txt\",\n",
    "        \"book_name\": \"Hamlet\",\n",
    "        \"target_speaker\": \"Hamlet\",\n",
    "        \"is_narrator\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in books:\n",
    "    print(f\"\\n === Processing {config['book_name']}...\")\n",
    "\n",
    "    pipeline = BookProcessingPipeline(\n",
    "        raw_txt_path=pathlib.Path(config[\"raw_txt\"]),\n",
    "        book_name=config[\"book_name\"],\n",
    "        target_speaker=config[\"target_speaker\"],\n",
    "        is_narrator=config[\"is_narrator\"]\n",
    "    )\n",
    "    pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
